{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-06T12:41:54.828355Z",
     "start_time": "2025-11-06T12:41:51.416200Z"
    }
   },
   "source": [
    "import chromadb\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel, AutoTokenizer,AutoModelForCausalLM,TrainingArguments, DataCollatorForLanguageModeling, Trainer\n",
    ")\n",
    "\n",
    "from week5.tools.get_embedding import get_embedding\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:41:54.913597Z",
     "start_time": "2025-11-06T12:41:54.851314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qwen_model_path = '/home/wby/projects/model/Qwen-7B-Chat'\n",
    "db_path = '/home/wby/projects/week5/chroma_db/chroma_recipe_db'\n",
    "model_path = '/home/wby/projects/model/all-MiniLM-L6-v2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "62aa2ecf0aab0e71",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:41:55.664503Z",
     "start_time": "2025-11-06T12:41:54.970440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_qwen_tokenizer(qwen_model_path):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            qwen_model_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        qwen_official_chat_template = \"\"\"\n",
    "        {% for message in messages %}\n",
    "            {% if message['role'] == 'user' %}\n",
    "                <|im_start|>user\n",
    "                {{ message['content'] }}\n",
    "                <|im_end|>\n",
    "            {% elif message['role'] == 'assistant' %}\n",
    "                <|im_start|>assistant\n",
    "                {{ message['content'] }}\n",
    "                <|im_end|>\n",
    "            {% endif %}\n",
    "        {% endfor %}\n",
    "        {% if add_generation_prompt %}\n",
    "            <|im_start|>assistant\n",
    "        {% endif %}\n",
    "        \"\"\"\n",
    "        tokenizer.chat_template = qwen_official_chat_template\n",
    "        # é…ç½®pad_tokenï¼ˆå’Œä¹‹å‰ä¸€è‡´ï¼‰\n",
    "        if tokenizer.eos_token is None:\n",
    "            tokenizer.eos_token = \"<|endoftext|>\"\n",
    "            tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        print(\"åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼\")\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"åˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼š{e}\")\n",
    "        return None\n",
    "\n",
    "qwen_tokenizer = load_qwen_tokenizer(qwen_model_path)"
   ],
   "id": "83a505b037e325cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:41:55.689204Z",
     "start_time": "2025-11-06T12:41:55.677675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "def load_qwen_model(qwen_model_path):\n",
    "    try:\n",
    "        print(\"å¼€å§‹åŠ è½½æ¨¡å‹ï¼Œæ¨¡å‹è¾ƒå¤§ï¼Œè€å¿ƒç­‰å¾…ï¼Œç¥ˆç¥·æ²¡äººå ç”¨GPU\")\n",
    "        # 1. å®šä¹‰4ä½é‡åŒ–é…ç½®ï¼ˆæ›¿ä»£åŸæ¥çš„load_in_4bit=Trueï¼‰\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,  # å¯ç”¨4ä½é‡åŒ–\n",
    "            bnb_4bit_use_double_quant=True,  # åŒé‡åŒ–ï¼ˆè¿›ä¸€æ­¥å‹ç¼©ï¼‰\n",
    "            bnb_4bit_quant_type=\"nf4\",  # ç”¨NF4é‡åŒ–ï¼ˆQLORAæ¨èï¼‰\n",
    "            bnb_4bit_compute_dtype=torch.float16  # è®¡ç®—æ—¶ç”¨16ä½ç²¾åº¦\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            qwen_model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,  # 16ä½ç²¾åº¦ï¼ˆæ ¸å¿ƒä¿®æ”¹1ï¼šå»æ‰load_in_4bitï¼‰\n",
    "            # load_in_4bit=True,  # QLORAçš„é‡åŒ–é…ç½®ï¼ŒLoRAéœ€åˆ é™¤\n",
    "            device_map=\"auto\"\n",
    "        ).eval()\n",
    "        print('Qwen-7B-Chat æ¨¡å‹åŠ è½½æˆåŠŸ')\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"out of memory\" in error_msg.lower():\n",
    "            print(f\"âŒ æ˜¾å­˜ä¸è¶³ï¼è§£å†³æ–¹æ¡ˆï¼š1. æ”¹ç”¨ CPU åŠ è½½ï¼ˆæŠŠ device_map è®¾ä¸º 'cpu'ï¼‰ï¼›2. ç”¨ Int4 é‡åŒ–æ¨¡å‹ï¼ˆéœ€è£… auto-gptqï¼‰\")\n",
    "        elif \"No such file or directory\" in error_msg:\n",
    "            print(f\"âŒ æ¨¡å‹è·¯å¾„é”™è¯¯ï¼è¯·ç¡®è®¤ {model_path} ä¸‹æœ‰ config.jsonã€pytorch_model-*.bin ç­‰æ–‡ä»¶\")\n",
    "        else:\n",
    "            print(f\"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{error_msg}\")\n",
    "        return None\n"
   ],
   "id": "b8c0fbab5964bcb9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:42:07.959639Z",
     "start_time": "2025-11-06T12:41:55.739503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if qwen_tokenizer:\n",
    "    qwen_model = load_qwen_model(qwen_model_path)\n",
    "else:\n",
    "    print('åˆ†è¯å™¨æœªåŠ è½½ï¼Œæ— æ³•åŠ è½½æ¨¡å‹')"
   ],
   "id": "bc1cc2ddb3fef2ae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹åŠ è½½æ¨¡å‹ï¼Œæ¨¡å‹è¾ƒå¤§ï¼Œè€å¿ƒç­‰å¾…ï¼Œç¥ˆç¥·æ²¡äººå ç”¨GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0217bdc2f6b4626aa6819b33a467d8f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-7B-Chat æ¨¡å‹åŠ è½½æˆåŠŸ\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:42:08.189372Z",
     "start_time": "2025-11-06T12:42:08.037046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 3. é…ç½®QLORA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],  # QWenæ³¨æ„åŠ›å±‚\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "# è£…LoRAé€‚é…å™¨\n",
    "qwen_model = get_peft_model(qwen_model, lora_config)\n",
    "qwen_model.print_trainable_parameters()  # ä»è¾“å‡º~0.01%å¯è®­ç»ƒå‚æ•°"
   ],
   "id": "8cc6740812992e33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 7,725,518,848 || trainable%: 0.0543\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:42:08.226606Z",
     "start_time": "2025-11-06T12:42:08.206262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# æ£€æŸ¥æ¨¡å‹æ‰€æœ‰å‚æ•°çš„ requires_grad çŠ¶æ€\n",
    "print(\"æ¨¡å‹å‚æ•°çŠ¶æ€ï¼ˆå‰10ä¸ªå‚æ•°ç¤ºä¾‹ï¼‰ï¼š\")\n",
    "for name, param in list(qwen_model.named_parameters())[:10]:\n",
    "    print(f\"å‚æ•°åï¼š{name:50} | æ˜¯å¦å¯è®­ç»ƒï¼š{param.requires_grad} | å½¢çŠ¶ï¼š{param.shape}\")\n",
    "\n",
    "# ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°æ•°é‡\n",
    "trainable_params = [p for p in qwen_model.parameters() if p.requires_grad]\n",
    "print(f\"\\nå¯è®­ç»ƒå‚æ•°æ€»æ•°ï¼š{len(trainable_params)}\")\n",
    "print(f\"å¯è®­ç»ƒå‚æ•°å æ¯”ï¼š{len(trainable_params)/len(list(qwen_model.parameters()))*100:.6f}%\")\n",
    "# æ£€æŸ¥QLORAé€‚é…å™¨æ˜¯å¦ç”Ÿæ•ˆ\n",
    "qwen_model.print_trainable_parameters()  # åº”è¾“å‡º ~0.01% çš„å¯è®­ç»ƒå‚æ•°\n",
    "\n",
    "# æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸º PeftModel ç±»å‹ï¼ˆç¡®ä¿é€‚é…å™¨å·²å®‰è£…ï¼‰\n",
    "print(f\"æ¨¡å‹ç±»å‹ï¼š{type(qwen_model)}\")  # åº”è¾“å‡º <class 'peft.peft_model.PeftModelForCausalLM'>"
   ],
   "id": "2d02b403eaf9f4fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹å‚æ•°çŠ¶æ€ï¼ˆå‰10ä¸ªå‚æ•°ç¤ºä¾‹ï¼‰ï¼š\n",
      "å‚æ•°åï¼šbase_model.model.transformer.wte.weight            | æ˜¯å¦å¯è®­ç»ƒï¼šFalse | å½¢çŠ¶ï¼štorch.Size([151936, 4096])\n",
      "å‚æ•°åï¼šbase_model.model.transformer.h.0.ln_1.weight       | æ˜¯å¦å¯è®­ç»ƒï¼šFalse | å½¢çŠ¶ï¼štorch.Size([4096])\n",
      "å‚æ•°åï¼šbase_model.model.transformer.h.0.attn.c_attn.base_layer.weight | æ˜¯å¦å¯è®­ç»ƒï¼šFalse | å½¢çŠ¶ï¼štorch.Size([12288, 4096])\n",
      "å‚æ•°åï¼šbase_model.model.transformer.h.0.attn.c_attn.base_layer.bias | æ˜¯å¦å¯è®­ç»ƒï¼šFalse | å½¢çŠ¶ï¼štorch.Size([12288])\n",
      "å‚æ•°åï¼šbase_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight | æ˜¯å¦å¯è®­ç»ƒï¼šTrue | å½¢çŠ¶ï¼štorch.Size([8, 4096])\n",
      "å‚æ•°åï¼šbase_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight | æ˜¯å¦å¯è®­ç»ƒï¼šTrue | å½¢çŠ¶ï¼štorch.Size([12288, 8])\n",
      "å‚æ•°åï¼šbase_model.model.transformer.h.0.attn.c_proj.weight | æ˜¯å¦å¯è®­ç»ƒï¼šFalse | å½¢çŠ¶ï¼štorch.Size([4096, 4096])\n",
      "å‚æ•°åï¼šbase_model.model.transformer.h.0.ln_2.weight       | æ˜¯å¦å¯è®­ç»ƒï¼šFalse | å½¢çŠ¶ï¼štorch.Size([4096])\n",
      "å‚æ•°åï¼šbase_model.model.transformer.h.0.mlp.w1.weight     | æ˜¯å¦å¯è®­ç»ƒï¼šFalse | å½¢çŠ¶ï¼štorch.Size([11008, 4096])\n",
      "å‚æ•°åï¼šbase_model.model.transformer.h.0.mlp.w2.weight     | æ˜¯å¦å¯è®­ç»ƒï¼šFalse | å½¢çŠ¶ï¼štorch.Size([11008, 4096])\n",
      "\n",
      "å¯è®­ç»ƒå‚æ•°æ€»æ•°ï¼š64\n",
      "å¯è®­ç»ƒå‚æ•°å æ¯”ï¼š19.814241%\n",
      "trainable params: 4,194,304 || all params: 7,725,518,848 || trainable%: 0.0543\n",
      "æ¨¡å‹ç±»å‹ï¼š<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:42:08.325258Z",
     "start_time": "2025-11-06T12:42:08.257147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# ä»…è·å–å¯è®­ç»ƒå‚æ•°ï¼ˆQLORAçš„ä½ç§©çŸ©é˜µå‚æ•°ï¼‰\n",
    "trainable_params = [p for p in qwen_model.parameters() if p.requires_grad]\n",
    "\n",
    "# å®šä¹‰åŸç”Ÿ AdamW ä¼˜åŒ–å™¨ï¼ˆä¸ç»è¿‡ accelerate å°è£…ï¼‰\n",
    "optimizer = AdamW(\n",
    "    trainable_params,\n",
    "    lr=2e-4,  # å­¦ä¹ ç‡å’Œä¹‹å‰ä¿æŒä¸€è‡´\n",
    "    weight_decay=0.01  # å¯é€‰ï¼šæ·»åŠ æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    ")"
   ],
   "id": "302d1237a1de26c5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:43:07.241209Z",
     "start_time": "2025-11-06T12:42:08.340237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer  # å…³é”®ï¼šå¯¼å…¥SFTTrainer\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "json_data_path = '/home/wby/projects/week5/data/food_task.json'\n",
    "output_dir = '/home/wby/projects/week5/data/qwen_food_lora' #å­˜æ”¾QLORAé€‚é…å™¨çš„åœ°æ–¹\n",
    "\n",
    "# -------------------------- æ–°å¢ï¼šçŒ´å­è¡¥ä¸ï¼ˆå…³é”®ä¿®å¤ï¼‰ --------------------------\n",
    "from torch.optim import AdamW\n",
    "from trl import SFTTrainer  # å…³é”®ï¼šå¯¼å…¥SFTTrainer\n",
    "\n",
    "# ç»™ AdamW åŠ¨æ€æ·»åŠ ä¸€ä¸ªç©ºçš„ train() æ–¹æ³•ï¼Œé¿å… AttributeError\n",
    "def dummy_train(self):\n",
    "    pass  # ä»€ä¹ˆéƒ½ä¸åšï¼Œä»…ä¸ºäº†é¿å…æŠ¥é”™\n",
    "\n",
    "# æŠŠç©ºæ–¹æ³•ç»‘å®šåˆ° AdamW ç±»ä¸Š\n",
    "if not hasattr(AdamW, \"train\"):\n",
    "    AdamW.train = dummy_train\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 4. åŠ è½½å¹¶æ ¼å¼åŒ–æ•°æ®é›†ï¼ˆSFTTraineréœ€æŒ‡å®šâ€œæ–‡æœ¬åˆ—â€ï¼Œç®€åŒ–æ ¼å¼ï¼‰\n",
    "def format_dataset(example):\n",
    "    # æ‹¼æ¥â€œæŒ‡ä»¤+è¾“å…¥+è¾“å‡ºâ€ä¸ºå•æ–‡æœ¬ï¼ˆSFTTraineré»˜è®¤è¯»å–â€œtextâ€åˆ—ï¼‰\n",
    "    example[\"text\"] = f\"\"\"### æŒ‡ä»¤ï¼š{example['instruction']}\n",
    "### è¾“å…¥ï¼š{example['input']}\n",
    "### è¾“å‡ºï¼š{example['output']}\"\"\"\n",
    "    return example\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=json_data_path)[\"train\"]\n",
    "formatted_dataset = dataset.map(format_dataset, remove_columns=dataset.column_names)\n",
    "\n",
    "# 5. é…ç½®è®­ç»ƒå‚æ•°ï¼ˆæ–°å¢ä¼˜åŒ–å™¨æŒ‡å®šï¼Œå…³é”®ä¿®å¤ï¼‰\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    # fp16=True,  # QLORAéœ€å¼€å¯æ··åˆç²¾åº¦ï¼ŒLoRAç”¨16ä½æ¨¡å‹å¯åˆ é™¤ï¼ˆä¹Ÿå¯ä¿ç•™ï¼‰\n",
    "    optim=\"adamw_torch\"  # ç›´æ¥ç”¨åŸç”ŸAdamWï¼Œä¸ä¼šå†æŠ¥train()é”™\n",
    ")\n",
    "\n",
    "# ç”¨SFTTrainerï¼ˆæ¨èï¼Œæµç¨‹ç®€å•ï¼‰\n",
    "trainer = SFTTrainer(\n",
    "    model=qwen_model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=qwen_tokenizer,\n",
    "    max_seq_length=128,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\"\n",
    ")\n",
    "trainer.train()  # æ­¤æ—¶ä¸ä¼šå†æŠ¥ AttributeErrorï¼\n",
    "\n",
    "# 8. ä¿å­˜é€‚é…å™¨ï¼ˆå’Œä¹‹å‰ä¸€è‡´ï¼‰\n",
    "qwen_model.save_pretrained(f\"{output_dir}/final_lora\")\n",
    "print(f\"å¾®è°ƒå®Œæˆï¼é€‚é…å™¨ä¿å­˜åœ¨ï¼š{output_dir}/final_lora\")"
   ],
   "id": "9214b1990411f75e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f77446b96e474044b4b57cad6c17c4da"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4758db060c894ac085555d0423528fae"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wby/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/wby/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/training_args.py:2063: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/wby/anaconda3/envs/pytorch/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/wby/anaconda3/envs/pytorch/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "924cee6395e046938844b991bafc8714"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wby/anaconda3/envs/pytorch/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:402: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 00:55, Epoch 18/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.103600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "05b770cc83c56a398c95ead78e4c3a78"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾®è°ƒå®Œæˆï¼é€‚é…å™¨ä¿å­˜åœ¨ï¼š/home/wby/projects/week5/data/qwen_food_lora/final_lora\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:43:07.523291Z",
     "start_time": "2025-11-06T12:43:07.519338Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "db42c08de44e8819",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
